{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READING DOCUMENTS FROM NEWSGROUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]             #storing text in each document\n",
    "classes=[]          #storing the class(newsgroup) of each document\n",
    "\n",
    "basepath = None       #Enter path of folder where data is stored\n",
    "os.chdir(basepath)\n",
    "folders=os.listdir(basepath)\n",
    "\n",
    "for folder in folders:\n",
    "    current_path=os.path.join(basepath,folder)\n",
    "    os.chdir(current_path)\n",
    "    for doc in os.listdir():\n",
    "        with open(os.path.join(current_path,doc),\"r\") as f:\n",
    "            docs.append(f.read())\n",
    "            classes.append(folder) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING AND MAKING VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "#list of stop words\n",
    "punc = list(string.punctuation)\n",
    "stops = stopwords.words('english')\n",
    "stops += punc\n",
    "\n",
    "#converting sentences to words\n",
    "words = [word_tokenize(doc) for doc in docs]\n",
    "\n",
    "#cleaning words\n",
    "\n",
    "def get_pos(tag):\n",
    "    \n",
    "    if(tag.startswith('J')):\n",
    "        return wordnet.ADJ\n",
    "    elif(tag.startswith('V')):\n",
    "        return wordnet.VERB\n",
    "    elif(tag.startswith('N')):\n",
    "        return wordnet.NOUN\n",
    "    elif(tag.startswith('R')):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_words = []\n",
    "for doc in words:\n",
    "    curr_doc = []\n",
    "    for w in doc:\n",
    "        if(w not in stops and len(w) > 3 and w.isalpha()):\n",
    "            tag = pos_tag([w])[0][1]\n",
    "            simple_word = lemmatizer.lemmatize(w, pos = get_pos(tag))\n",
    "            curr_doc.append(simple_word.lower())\n",
    "    clean_words.append(curr_doc)   \n",
    "\n",
    "\n",
    "#clean_words contains 2000 entries each corresponding to clean words in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting dictionary in descending order of words based on frquency into a list of tuples\n",
    "vocab=sorted(vocabulary.items(), key = lambda x : x[1], reverse=True)\n",
    "new_vocab=vocab[:20000]   #considering the top 25000 words as features\n",
    "features={}     #assigning an index to each word\n",
    "for c, i in enumerate(new_vocab):\n",
    "    features[i[0]]=c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "#converting words in a documents to sentences (for count vectorizer)\n",
    "new_document = [\" \".join(doc) for doc in clean_words]\n",
    "\n",
    "x_train, x_test, y_train, y_test = tts(new_document, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features = 20000, max_df = 0.6)    #max_df = 0.6 means that ignore words that come in more than 60% of the documents\n",
    "\n",
    "a = cv.fit_transform(x_train)\n",
    "x_train_documents = a.todense()\n",
    "x_test_documents = cv.transform(x_test).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting input data into a 2d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    \n",
    "    x_2d=np.zeros((len(x),len(features.keys())))\n",
    "    idx=0\n",
    "    for i in x:\n",
    "        for word in i.split():\n",
    "            if(word in features.keys()):\n",
    "                x_2d[idx][features[word]]+=1\n",
    "        idx+=1\n",
    "    \n",
    "    return x_2d\n",
    "\n",
    "x_train_new=transform(x_train)\n",
    "x_test_new=transform(x_test)\n",
    "y_train=np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INBUILT MULTINOMIAL NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "clf=MultinomialNB()\n",
    "clf.fit(x_train_new,y_train)\n",
    "y_pred=clf.predict(x_test_new)\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELF IMPLEMENTED MULTINOMIAL NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    \n",
    "    def __init__(self,x_train,y_train):\n",
    "        self.x=x_train\n",
    "        self.y=y_train\n",
    "        self.words={}\n",
    "        \n",
    "    def fit(self):\n",
    "\n",
    "        classes=set(self.y)\n",
    "        num_features=self.x.shape[1]\n",
    "        \n",
    "        for curr_class in classes:\n",
    "            true_rows = self.y == curr_class\n",
    "            x_curr=self.x[true_rows]\n",
    "            y_curr=self.y[true_rows]\n",
    "            self.words[curr_class]={}\n",
    "            self.words[curr_class][\"sum\"]=x_curr.sum()      #all words in the current class\n",
    "            self.words[curr_class][\"total\"]=len(y_curr)     #total number of inputs in current class\n",
    "            for curr_feature in range(num_features):\n",
    "                self.words[curr_class][curr_feature]=x_curr[:,curr_feature].sum()   #count of a word for current class\n",
    "                \n",
    "    def get_output(self,x):\n",
    "\n",
    "        best_proba = -100\n",
    "        best_class = -100\n",
    "        first = True\n",
    "        num_features=len(x)\n",
    "        classes=set(self.y)\n",
    "        \n",
    "        for curr_class in classes:\n",
    "            \n",
    "            output = np.log(self.words[curr_class][\"total\"])-np.log(len(self.y))\n",
    "            for i in range(num_features):\n",
    "                num=(self.words[curr_class][i]+1)\n",
    "                denom=self.words[curr_class][\"sum\"] + num_features\n",
    "                output += x[i]*(np.log(num) - np.log(denom))\n",
    "        \n",
    "            if( first or output>best_proba):\n",
    "                best_proba=output\n",
    "                best_class=curr_class\n",
    "                \n",
    "            first=False\n",
    "\n",
    "        return best_class\n",
    "\n",
    "    def predict(self,x_test):\n",
    "\n",
    "        y_pred=[]\n",
    "        for x in x_test:\n",
    "            output_class=self.get_output(x)\n",
    "            y_pred.append(output_class)\n",
    "    \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1=MultinomialNaiveBayes(x_train_new,y_train)\n",
    "clf1.fit()\n",
    "y_pred1 = clf1.predict(x_test_new)\n",
    "print(classification_report(y_test,y_pred1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
